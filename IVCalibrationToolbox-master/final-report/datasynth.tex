\section{Data synthesis}
\label{Datasynth}
To test our system, we built two simulators to synthesize input data for the filter. The first simulator creates a list of translational accelerations and rotational velocities for the system. For example, a very simple list could be constant values for all accelerations and angular velocities. Another option would be to create a list of varying values based on sinesoidal waves at different frequences. The synthesized inertial data is integrated forward in time to generate the ground truth path for the IMU. At each time step, 3D points are projected into the camera for the ground truth pixel coordinates. The ground truth inertial and camera data are perturbed by Gaussian noise and a random walk bias term and used as the inputs for the Kalman filter. As we will explain next, generating a specific output trajectory such as a figure-eight is very difficult to do using only inertial inputs.

We noticed that the above method produces a camera path where the camera does not consistently look at the points. The camera image plane might intersect the landmarks in world space, in which case, the points projections would lie at infinity. The camera may unrealistically look away from the points, or it may go too far from them. We needed a method that would produce a reasonable camera path, where the camera is within reach of the landmarks, and is consistently looking at them. In our second synthesis method, we manually defined control points of a Bezier curve around the $3D$ points, and we generated a smooth camera path using the Bezier curve. This path defined the position of the camera, $\bb{p}_C^W(t)$ at different instants of time. To specify the orientation $\hat{\overline{q}}_C^W(t)$, we computed a look at vector $v_{lookat}$ from the camera position $\bb{p}_C^W$ to the mean of the landmarks, $\bb{p}_{\mu}^C$. We defined an up vector, $v_{up}$ to lie along the $z$-axis by default, and we computed a third orthogonal vector, $v_{orth}=v_{lookat} \times v_{up}$. We computed the orientation in matrix form as $\begin{bmatrix}v_{orth} &v_{up}& v_{lookat}\end{bmatrix}$, and we converted it to the quaternion $\hat{\overline{q}}_C^W(t)$. Figure \ref{synthcontrolpts} shows the control points specified for the camera path, and Figure \ref{synthpath} shows the camera path as it looks at the points, the IMU attached rigidly to the camera, and the projected points in the camera's image plane.
\begin{figure}[t]
\centering
\includegraphics[width=.5\linewidth]{synth_controlpts.eps}
\caption{Top view of landmarks and control points used to generate camera path}
\label{synthcontrolpts}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{synth_path.eps}
\caption{Left: Camera path in the world space in black, landmarks in green, IMU-camera setup in blue, camera rotation matrix (look-at, up, and orthogonal vectors) in red. Right: projections of the landmarks in the image plane of the camera.}
\label{synthpath}
\end{figure}